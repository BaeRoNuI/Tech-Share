이번 주제에서는 MySQL를 사용하여 천만개의 데이터를 기준으로 전체 데이터를 조회하는 경우에서 Offset를 왜 사용하지 않아야 하는지 알아보도록 하겠습니다.

# Data 생성

온라인 서점에서 전체 검색으로 책을 검색한다고 가정하기 위해 BOOK이라는 테이블을 생성합니다.

## Table 구조

```sql
CREATE TABLE BOOK (
    ID BIGINT AUTO_INCREMENT PRIMARY KEY,
    NAME VARCHAR(255),
    AUTHOR VARCHAR(255)
    CREATED_DATE TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

## 인덱스 생성

ID로 정렬되고 BOOK NAME만 가져올 것이기 때문에 NAME에 대해 인덱스를 생성해줍니다.

```sql
CREATE INDEX idx_book_name ON BOOK (NAME);
```

## Data Insert Code

총 천만개의 데이터를 Insert합니다. 데이터는 Faker를 통해 랜덤 데이터를 생성합니다.

```python
import mysql.connector
from faker import Faker

# MySQL 연결 설정
conn = mysql.connector.connect(
    host='localhost',
    user='root',
    password='admin',
    database='TEST'
)

cursor = conn.cursor()

# BOOK 테이블 생성
create_table_query = """
CREATE TABLE IF NOT EXISTS BOOK (
    ID BIGINT AUTO_INCREMENT PRIMARY KEY,
    NAME VARCHAR(255),
    AUTHOR VARCHAR(255),
    CREATED_AT TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
"""
cursor.execute(create_table_query)
conn.commit()

# Faker 인스턴스 생성
fake = Faker()

# 더미 데이터 생성 및 삽입
insert_query = "INSERT INTO BOOK (NAME, AUTHOR) VALUES (%s, %s)"

batch_size = 10000
total_records = 100000000

for i in range(0, total_records, batch_size):
    data = [(fake.text(max_nb_chars=20), fake.name()) for _ in range(batch_size)]
    cursor.executemany(insert_query, data)
    conn.commit()
    print(f"Inserted {i + batch_size} records")

# 연결 종료
cursor.close()
conn.close()

```

# Offset 기반 페이징

10번째 페이지에 해당하는 100개의 데이터를 가져오기 위한 Offset 기반 페이징은 아래와 같은 쿼리로 할 수 있습니다.

```sql
SELECT NAME, ID
FROM BOOK
ORDER BY ID DESC
LIMIT 100 OFFSET 900
```

## 어떤 문제가 발생할 수 있을까?

- 대량의 데이터에서 속도가 느립니다.
  - Offset이 작은 경우에는 사실 빠르게 동작합니다. 하지만 Offset이 크기가 커질수록 기하급수적으로 많이 느려집니다.
- 중복데이터를 포함할 수 있습니다.
  - 검색 중간에 Insert나 update Query가 발생하면 중복데이터를 읽어올 수 있는 상황이 발생할 수 있습니다.
- 이러한 문제는 Offset 기반 Query의 동작방식 때문인데 한번 알아보도록 합시다.

## Offset 기반 Query 동작방식

만약 10번째 페이지에 해당하는 100개의 데이터를 가져온다고 가정하면 아래와 같은 방식으로 동작합니다.

1. 위와 같은 쿼리에서는 `ORDER BY ID DESC` 조건에 따라 Offset을 처리하기 위한 데이터들을 가져와 정렬합니다.
2. 정렬된 데이터 중 앞의 900개의 레코드를 건너뜁니다.
3. 건너뛴 후 나머지 데이터 중에서 100개의 레코드를 선택합니다. 즉, 901번째 레코드부터 1000번째 레코드까지의 데이터를 가져옵니다.

## 왜 문제가 발생하나?

- 데이터베이스는 정렬 조건에 따라 전체 데이터를 정렬해야 하므로 많은 CPU 리소스를 소모합니다.
- Offset 값이 클수록 불필요한 데이터 로드 및 처리가 증가합니다. 예를 들어, Offset이 1000이라면, 1000개의 데이터를 무시하고 나머지 데이터를 선택해야 하므로 많은 메모리를 사용하고 I/O 작업이 발생할 수 있어 많은 시간이 소요됩니다.
- Offset 기반 페이징은 데이터가 정렬된 상태에서 작동하므로, 데이터가 추가되거나 삭제될 때 예상치 못한 중복 데이터나 누락 데이터가 발생할 수 있습니다.

# 커서 기반 페이징

커서 기반 페이징은 데이터베이스에서 대량의 데이터를 효율적으로 페이지네이션하는 방법 중 하나로, 특정 기준 값(커서)을 사용하여 다음 페이지의 시작 지점을 지정하는 방식입니다. 이 방식은 특히 대량의 데이터가 있는 경우 성능이 좋습니다.

### 예시

첫번째 페이지에서 데이터를 가져옵니다.

```sql
SELECT NAME, ID
FROM BOOK
ORDER BY ID DESC
LIMIT 100;
```

**다음 페이지 가져오기** 앞에서 가져온 데이터 중 마지막 레코드의 값을 사용합니다. 정상적인 동작을 하기 위해서는 해당 값은 고유하면서 ORDER BY의 기준이 되는 값이어야합니다. 아래와 같은 경우는 마지막 레코드의 ID로 설정했습니다.

마지막 ID가 101이라면, 이를 커서로 사용하여 다음 페이지를 가져옵니다.

```sql
SELECT NAME, ID FROM BOOK WHERE id < 101 LIMIT 10;
```

### 장점

- 커서 기반 페이징은 인덱스를 사용하여 특정 위치에서 바로 검색을 시작하므로, 매우 빠른 성능을 제공합니다.
- 메모리를 많이 사용하지 않습니다.
- 데이터가 추가되거나 삭제되더라도, 커서를 사용하여 일관된 결과를 반환할 수 있습니다.

### 단점

- 구현이 다소 복잡할 수 있으며, 각 페이지 요청마다 마지막 ID를 저장하고 관리해야 합니다.
- 커서를 구현하기 어려운 복잡한 쿼리에서는 사용하기 힘들 수 있습니다.

## Offset 기반 페이징과 커서 기반 페이징 성능 비교

### 테스트 환경

1000만개의 데이터가 있는 BOOK 테이블

### **Offset 기반 페이징**

- 9999900번째 행에서 100개의 행을 가져오는 데
  - 평균 2초 소요.
  - Buffer Pool Data Pages : 8000개
- 100만개의 행을 가져오는 데
  - 평균 12분 소요
  - CPU 사용량 1/10 사용(10 Core 기준)

### **커서 기반 페이징**

- 9999900번째 행에서 100개의 행을 가져오는 데
  - 평균 0.04초 소요.
  - Buffer Pool Data Pages : 2000개
- 100만개의 행을 가져오는 데
  - 평균 4초 소요
  - CPU 사용량 1/1000 사용(10 Core 기준)
    - 거의 사용되지 않음

### 사용한 코드

Offset 방식

```python
import mysql.connector
import time

def fetch_with_offset(limit, total_records):

    start_time = time.time()

    conn = mysql.connector.connect(
        host='localhost',
        user='root',
        password='admin',
        database='TEST'
    )
    cursor = conn.cursor()

    all_data = []
    total_pages = (total_records // limit) + (1 if total_records % limit > 0 else 0)
    for page in range(total_pages):
        offset = page * limit
        query = """
        SELECT NAME, ID
        FROM BOOK
        ORDER BY ID DESC
        LIMIT %s OFFSET %s;
        """
        cursor.execute(query, (limit, offset))
        cursor.fetchall()
        print(f"Fetched page {page + 1}/{total_pages}")

    query = "SELECT DATABASE_PAGES FROM INFORMATION_SCHEMA.INNODB_BUFFER_POOL_STATS;"
    cursor.execute(query)

        # 결과 가져오기
    rows = cursor.fetchall()

        # 결과 출력
    for row in rows:
      print(f"PAGES_DATA: {row[0]}")
    cursor.close()
    conn.close()

    end_time = time.time()
    total_time = end_time - start_time
    print(f"OFFSET 실행 시간: {total_time} 초")

    return all_data

# 실행 예시
limit = 100
total_records = 1000000  # 1000만 개의 데이터
fetch_with_offset(limit, total_records)

```

Cursor based

```python
import mysql.connector
import time

def fetch_with_cursor_pagination(limit, total_records):

    start_time = time.time()

    conn = mysql.connector.connect(
        host='localhost',
        user='root',
        password='admin',
        database='TEST'
    )
    cursor = conn.cursor()

    all_data = []
    last_id = None
    page = 0
    total_pages = (total_records // limit) + (1 if total_records % limit > 0 else 0)

    while page < total_pages:
        if last_id:
            query = """
            SELECT NAME, ID
            FROM BOOK
            WHERE ID < %s
            ORDER BY ID DESC
            LIMIT %s;
            """
            cursor.execute(query, (last_id, limit))
        else:
            query = """
            SELECT NAME, ID
            FROM BOOK
            ORDER BY ID DESC
            LIMIT %s;
            """
            cursor.execute(query, (limit,))

        results = cursor.fetchall()

        print(f"Fetched page {page + 1}/{total_pages}")

        if not results:
            break

        all_data.extend(results)
        last_id = results[-1][1]  # 마지막 ID 값을 갱신
        page += 1

    query = "SELECT DATABASE_PAGES FROM INFORMATION_SCHEMA.INNODB_BUFFER_POOL_STATS;"
    cursor.execute(query)

        # 결과 가져오기
    rows = cursor.fetchall()

        # 결과 출력
    for row in rows:
      print(f"PAGES_DATA: {row[0]}")
    end_time = time.time()
    total_time = end_time - start_time
    print(f"총 실행 시간: {total_time} 초")
    cursor.close()
    conn.close()

    return all_data

# 실행 예시
limit = 100
total_records = 1000000  # 1000만 개의 데이터
all_data = fetch_with_cursor_pagination(limit, total_records)

# 데이터 확인 (여기서는 처음 10개의 데이터만 출력)
for row in all_data[:10]:
    print(row)

```

## 결론

Offset방식은 구현이 간단하기 때문에 적은 데이터셋에서는 Offset 방식을 사용하는 방법도 나쁘지않습니다.

하지만 대량 데이터를 페이징할 때 Offset 기반 페이징은 성능 저하와 메모리 사용량 증가 문제를 일으킵니다.

커서 기반 페이징은 성능과 메모리 사용 측면에서 훨씬 효율적이며, 대규모 데이터셋에서도 일관된 성능을 제공합니다. 따라서, 대량 데이터 페이징 시에는 가능하다면 Offset 기반 페이징보다 커서 기반 페이징을 사용하는 것이 좋습니다.
